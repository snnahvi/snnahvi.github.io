import "./projectBodies.css";
import Zoom from "react-medium-image-zoom";
import "react-medium-image-zoom/dist/styles.css";

const ProjectBody01 = () => {
	return (
		<div>
			<em style={{ fontSize: "15px" }}>Last updated: 1 Nov, 2025</em>
			<div style={{ margin: "0 auto", textAlign: "center" }}>
				<figure>
					<Zoom>
						<img
							alt="SpartanX Logo"
							src="../project_Gamification.png"
							width="200"
						/>
					</Zoom>
					<figcaption style={{ fontSize: "15px" }}>
						<em>Building... </em>
					</figcaption>
				</figure>
			</div>
			Building....
			{/* “Is it possible?” This was the first question every person asked me when I explained what we do in our
            startup. At times, I felt hopeless about the results of our work. However, the primary reason we persevered
            in our struggle was the realization that major companies, such as <a target="_blank" rel="noreferrer"
                                                                                 href="https://www.rentec.com/Home.action?index=true">RenTech</a> hedge
            fund and <a target="_blank" rel="noreferrer"
                        href="https://www.blackrock.com/corporate">BlackRock</a> investment management, have
            successfully employed similar approaches, serving as examples of success.
            <br/><br/>
            In Cardano Trader, our goal was to create a fully automated system for managing assets in the Tehran Market
            using AI, particularly ML and DL methods. I dedicated two and a half years to this endeavor, collaborating
            with two friends who joined me later. We worked day and night on this project. Now, with two years having
            passed since the failure of our startup and the expiration of our NDA, I aim to share some insights about
            our journey, highlighting both the challenges and successes we experienced in implementing this idea in the
            real world.
            <br/><br/>
            The system comprises three main components: data listener, an AI core, and an accounts manager.
            The data listener component functioned as a scraper, collecting technical data primarily from two
            sources—the market's official page and the broker's page. This part's responsibility was to publish data in
            memory, and to minimize delays, the data was directly written to <a target="_blank" rel="noreferrer"
                                                                                href="https://www.softprayog.in/programming/interprocess-communication-using-system-v-shared-memory-in-linux#:~:text=Shared%20memory%20is%20one%20of,the%20message%20queues%20and%20semaphores.">shared
            memory</a>. The
            AI core served as the hub for various algorithms employing different strategies. These
            algorithms determined, at each time frame, the selection of symbols (where we could invest our money in
            different symbols in the market) and the portion of the entire asset to be allocated. We implemented three
            various algorithms for this segment, with SpartanX, representing a fine-tuned version of
            Spartan, being one of them. Lastly, the accounts manager, also a scraper, managed orders generated by each
            algorithm in the AI core,
            facilitating their submission, modification, or cancellation on the exchange page for multiple accounts.
            <br/><br/>
            In this article, I aim to elucidate SpartanX and the concept behind it. I won’t delve into the other
            components and algorithms, as they warrant separate articles. Initially, I will delve into the origin of
            the SpartanX idea by presenting results from its precursor concepts. Then, I will provide a detailed
            explanation. Finally, I will present real-world test results and conclude the article by addressing
            practical issues.
            <br/><br/>
            <div className="heading-1">Before Going Deeper</div>
            <br/><br/>
            It is worthwhile to mention that in real-world applications, there are many things you need to do beyond
            just loading data and training a model. Consider the following aspects to be addressed in a full data
            science project: gathering and cleaning data, reconstructing missing data, hardware requirements,
            versioning and checkpointing the models, visualization, and validation, real-time monitoring, testing, and
            retraining based on test results.
            <br/><br/>
            <div className="heading-1">About Tehran Market</div>
            <br/><br/>
            The Tehran Stock Exchange is Iran’s largest stock market, with an estimated daily transaction value of about
            $120 million. At the time we developed our system, there were approximately 750 active companies, and their
            shares could be traded as symbols.
            <br/><br/>
            The market has some restrictions and rules that pose challenges for a data scientist. For example, the
            market opens at 9 AM and closes at 12:30 PM, although the opening hours have occasionally changed.
            Additionally, each day, each symbol must fluctuate within a pre-determined range based on yesterday's
            closing price. This range varies for different symbols. Moreover, trading for each symbol must halt due to
            regulatory requirements, leading to the blocking of all assets. Designing a system must take these scenarios
            into consideration.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="SpartanX Logo"
                            src="../projects_spartan_x_02.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The price of the symbol DELOR in Rials (the currency of Iran) is depicted from mid-April to
                        mid-October 2020. The price is represented by dots for different days, with colors indicating
                        positive/negative trends—red, white, or green. White rectangles indicate the price limits for
                        each day. The red and grey vertical areas represent days when the symbol's trade is restricted.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Start: Prediction of the Future of Each Symbol</div>
            <br/><br/>
            The initial idea involves predicting the future price of each symbol and subsequently selecting the best
            symbols based on both their anticipated future and the confidence level of these predictions. For instance,
            if we estimate that symbol X's price will increase by 5% in 7 days with a confidence level of 70%, and
            symbol Y's price will rise by 3.5% in 7 days with a confidence level of 80%, we can allocate our assets
            among these two symbols according to our risk threshold. Another consideration is reserving a portion of our
            assets for potential future opportunities, a decision that is intricate and will be discussed later.
            <br/><br/>
            At the start of our startup, we trained various regression models and networks to predict the future price
            of each symbol. However, we encountered several challenges:
            <br/>
            <ul>
                <li>
                    Determining the prediction horizon: How many future points should be considered—next minute, next
                    hour, next day, next week, or next month?
                </li>
                <li>
                    Measuring prediction confidence: How can we quantify the confidence of our predictions?
                </li>
                <li>
                    Feature selection: What data should be included as features? Should additional economic measures
                    like MACD, EMA, and RSI be used?
                </li>
                <li>
                    Model training: Should a single model be trained for all symbols, or is it more effective to use
                    separate models for different symbols?
                </li>
                <li>
                    Non-stationary price history: The price history is non-stationary and varies over time and among
                    symbols. How can we address this variability?
                </li>
            </ul>
            <br/>
            Initially, we aggregated all symbols throughout their entire history to create a comprehensive dataset.
            Various features, such as MACD, were computed for each price and incorporated into our dataset. To address
            the non-stationary problem, we implemented a moving window of a specified size and utilized the min-max
            normalization method to maintain the price range within a specific interval. As for the model's response, we
            calculated the relative price change, representing the change in future price relative to the current price.
            <br/><br/>
            Despite the involvement of all symbols and a dataset size of approximately 200,000 samples, the abundance of
            features posed the curse of dimensionality problem for the model. To address this, we initially employed a
            Random Forest model to identify the most informative features, ultimately narrowing them down to about 20
            key features.
            <br/><br/>
            Over more than two months, we trained various models to predict future prices, but the results were
            consistently discouraging: almost always yielding predictions of zero. Regardless of changing both the model
            architecture and the dataset, the predictions remained stuck at zero. We explored a range of models,
            including RNN, LSTM, GRU, 1-dimensional CNN, Gaussian Process regression, and even Transformer.
            Additionally, we experimented with different selections of economic measures as features, various
            normalization methods, diverse time horizons from hours to weeks, and varied selections of data related to
            different dates in the market.
            <br/><br/>
            <div className="heading-1">What is the Meaning of Zero?</div>
            <br/><br/>
            We arrived at two possible explanations for our results. First, it might be a flawed assumption to aggregate
            all market data together. It is plausible that symbols' prices vary in distinct ways. For instance, Symbol
            X's price could rise under one specific condition, while under the same condition, the price of Y might
            drop. Moreover, this rule for a single symbol can change over time. The same condition that contributed to
            the rise in the price of Symbol Z two years ago may not have the same effect this year due to the overall
            market being in a harsh condition.
            <br/><br/>
            Secondly, we considered that the cause of price change within the time frame we examined might not be
            adequately represented in our features. For instance, if we aim to predict the price of Symbol Z in the next
            month, relying on daily trade value, current price, RSI, and similar features might be insufficient.
            Instead, the most significant determinant could be found in the net sales of the company. Unfortunately,
            thess types of fundamental data are not present in our dataset, and we do not have access to them.
            <br/><br/>
            <div className="heading-1">SpartanX</div>
            <br/><br/>
            So, we decided to shift our perspective and implemented three major changes. The first change involved
            time-dependent clustering of symbols. Our assumption was that the price dynamics of each symbol vary
            smoothly over time (with time referring to the day in this context). On any given day, all 750 symbols are
            grouped into 10 different clusters based on certain features extracted from their price fluctuations.
            <br/><br/>
            The second change involved approaching the problem as a classification problem rather than a regression one.
            In fact, accurately predicting the exact future price is challenging. When consulting an expert about
            a symbol, they often provide general directional insights, such as "It will go up" or "It will drop." In
            practice, the focus is more on understanding the overall trend rather than pinpointing the exact price.
            <br/><br/>
            Indeed, we labeled our data at each point as either a positive or negative sample. The label signifies that
            buying
            the share at that time point could result in a benefit. However, a challenge arises: At which point sell
            that share? How much benefit do you want to make and sell your share? Do you want to hold the share if the
            price drops, perhaps for a return? The classification of good/bad labels is highly dependent on the chosen
            trading strategy.
            <br/><br/>
            The third change involved reducing the prediction horizon to one hour. After thorough examination, I can
            confidently state that there is no substantial information in the technical data of the market for
            predicting the long-term future of a symbol, such as a month later. The market, being influenced by
            significant players, involves human decision-making based on several unpredictable factors. However, in a
            short time period, we observed that the price exhibits a discernible dynamic.
            <br/><br/>
            This entails a trade-off between profit and prediction ability. Lowering the future horizon allows for
            better price prediction, but short-term profits are typically modest. This is where the concept of
            High-Frequency Trading comes into play. In this method, numerous small trades are executed, and the
            cumulative result of these trades amounts to a significant profit.
            <br/><br/>
            Hence, SpartanX is divided into two interconnected sub-systems: the Signal Generator and the Stream Manager.
            The Signal Generator generates, every 2.5 seconds, a vector of probabilities for all symbols. Each entry in
            the vector represents the goodness of buying that symbol at that specific time (not predicting the future
            price
            value). The Stream Manager is responsible for managing the entire asset and utilizing it to invest in
            various symbols.
            <br/><br/>
            <div className="heading-1">Signal Generator</div>
            <br/><br/>
            At the beginning of each day, the Signal Generator extracts 5 features representing the price fluctuations
            of each symbol over the last 30 days. This results in a 5x30 matrix for each symbol. Then, we
            utilized <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/1802.03426">UMAP</a> to
            reduce these 150 features to 2 dimensions. Subsequently, we used <a target="_blank" rel="noreferrer"
                                                                                href="https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14">HDBScan</a> to
            categorize all 750 symbols into 10 clusters based on their reduced 2 features. This process is repeated
            daily, assigning each symbol to one of the 10 clusters. It is worth noting that a symbol may be in a
            different cluster today than it was 20 days ago. To account for variations in UMAP reduction and HDBScan
            clustering, we employed <a target="_blank" rel="noreferrer"
                                       href="https://en.wikipedia.org/wiki/Procrustes_transformation">Procrustes</a> to
            align these features for consecutive days.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Symbols Clustered"
                            src="../projects_spartan_x_03.gif"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Symbols are clustered, with each dot representing the reduced features of each symbol in 2
                        dimensions. Distinct colors correspond to each of the 10 clusters. The transition of clusters is
                        depicted in each frame of the animation, with dates annotated beneath the figure.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            We found the 10 clusters to be intriguing as they exhibited a high sensitivity to related symbols. For
            instance, the symbol Khodro consistently appeared as the closest symbol to Vsapa on most days. This
            alignment is meaningful as both companies are prominent automobile manufacturers and share significant
            similarities. It is noteworthy that we didn't explicitly feed this knowledge to our model; rather, it
            emerged
            based on price variability, effectively identifying and recovering related symbols.
            <br/><br/>
            Examining the price of sample symbols across various clusters revealed a compelling pattern. In essence, our
            approach enabled us to segregate symbols based on their liquidity. Less liquid symbols exhibited more
            significant price jumps, while more liquid symbols proved resistant to large price changes due to the
            substantial support from high volumes of buy and sell orders. This separation not only provided valuable
            insights into market dynamics but also paved the way for tailored trading strategies, recognizing that
            trading in different symbols necessitates distinct approaches contingent on the liquidity of each symbol.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Sample Price Clustered"
                            src="../projects_spartan_x_04.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Sample price changes for each cluster in a single day are illustrated, with the y-axis
                        representing the percentage change in price.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            Then, we trained different models for these clusters, recognizing that the price dynamics differ
            among them. The model's output, as discussed earlier, signifies the desirability of buying. A higher
            probability suggests a potentially larger profit when buying shares of that symbol at that time. For
            labeling data, we established a simple trading strategy: we buy a share at time 't' with price 'p.' If any
            of the following criteria are met, we sell that share:
            <br/>
            <ol>
                <li>
                    If the current price falls by less than 2% of the maximum price in the time window from time 't' to
                    the present.
                </li>
                <li>
                    If the price falls by less than 1% of 'p.'
                </li>
                <li>
                    If the current time surpasses 1 hour from time 't.'
                </li>
            </ol>

            <br/>
            We iterated through all time points (sampled with a 2.5-second period) in the dataset for each day and apply
            these criteria. If the sell price is higher than the buy price plus the exchange fee, that time point is
            labeled as 'good.' Conversely, the remaining points are labeled as 'bad.'
            <br/><br/>
            By training our model in this manner, we ensured that if the model accuracy is high, and we follow these
            predictions along with the aforementioned trading strategy, we can benefit from the market.
            <br/><br/>
            We utilized a <a target="_blank" rel="noreferrer"
                             href="https://arxiv.org/abs/1706.03762">Transformer</a> network
            for this purpose due to its foundation on the attention mechanism. Unlike
            RNN, which struggles with inferring patterns occurring at different time scales, the Transformer, leveraging
            Multi-Head Attention blocks, effectively addresses this limitation. To tailor the Transformer for our
            problem, we made specific adjustments to the architecture, such as removing embedding and modifying
            positional encoding.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Transformer Response"
                            src="../projects_spartan_x_05.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Sample predictions of the Transformer are depicted. The start of each red line signifies points
                        where the model predicted them as good places to invest (buy), while the end of the red line
                        indicates sell points according to the trade strategy.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Stream Manager</div>
            <br/><br/>
            In an imaginary scenario where we have access to a perfect model that predicts the future of each symbol
            with high confidence, say 90%, determining the best approach for investment becomes nuanced. While one might
            suggest investing all assets in the share of a symbol the first time, there is a 10% chance it could be a
            bad
            decision leading to losses. Alternatively, waiting for a better opportunity to buy another symbol and
            potentially gain greater benefits is also a consideration. Adding to the complexity, if we already hold
            shares of symbol X and it is currently in a loss, the decision of whether to sell those shares to free up
            assets for a symbol with a higher probability introduces another layer of complexity to the investment
            strategy.
            <br/><br/>
            In my opinion and based on my experience, the selection of these strategies holds more significance than
            predicting the future price. This aspect is closely tied to economic topics such as risk management and
            asset management.
            <br/><br/>
            We devised a simple yet effective strategy for this. Initially, our entire asset, let's say 1, is divided
            into a specified number of parts, denoted as 'streams,' let's say N. In each trade, we utilize one of these
            streams to buy and sell shares. At each time point, the system examines all available streams. If one of
            them is available, the Stream Manager consults the Signal Generator for promising symbols to buy. If the
            probability of a symbol exceeds a threshold and that symbol has not been purchased before, the stream is
            allocated to that symbol. The Stream Manager continuously monitors all active streams, and, as previously
            outlined, if any of these criteria are met, the shares of that symbol are sold:
            <br/>
            <ol>
                <li>
                    If the current price falls by less than 2% of the maximum price in the time window from time 't' to
                    the present.
                </li>
                <li>
                    If the price falls by less than 1% of 'p.'
                </li>
                <li>
                    If the current time surpasses 1 hour from time 't.'
                </li>
            </ol>

            <br/>
            The parameters of this method play a crucial role in determining the overall benefit. For example, if the
            number N is increased, the profit will decrease since the total profit is divided by N; simultaneously, the
            risk will decline. Therefore, finding the optimal set of parameters is highly impactful and poses a
            challenging problem. We identified the best set through grid searching across all possible combinations over
            a 3-week period, using five powerful machines with a Core i9 CPU. The following image illustrates the
            impact of changing some of these parameters on the overall profit.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Trading Parameters"
                            src="../projects_spartan_x_06.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The impact of varying distinct parameters of the trade strategy on daily profit is illustrated.
                        It is important to note that the profit can also be negative, emphasizing the potential
                        results of an incorrect choice of parameters, despite the Signal Generator perform well.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            Recall the labeling mechanism in the Signal Generator part. It is important to note that labels are
            dependent
            on this parameter set. The best parameter set and the profit are also influenced by the output of the Signal
            Generator. Yet, we should fine-tune these two sub-systems together. Fine-tuning these two sub-systems
            together is a non-trivial task, and currently, we fine-tune Transformer models after identifying the best
            set.
            <br/><br/>
            <div className="heading-1">Results and Discussion</div>
            <br/><br/>
            The entire structure underwent multiple tests. In a test spanning approximately one and a half months with
            an initial asset of $100,000, we achieved a 15% gain. Notably, in one day, two streams were bought and sold
            in a symbol within just 6 seconds. However, upon further analysis, we discovered that this profit is heavily
            influenced by market circumstances. High-frequency trading necessitates high liquidity as orders must be
            executed swiftly. Additionally, this strategy performs well in neutral markets, exhibiting effectiveness
            neither in a bullish nor a bearish trend.
            <br/><br/>
            This strategy encountered two practical issues during real tests, resulting in a decline in overall
            performance. Firstly, buy orders often failed to complete, primarily due to liquidity issues. Secondly, as
            mentioned earlier, the Tehran exchange market's rule stipulates a specific price range. Consequently, during
            a downward trend, shareholders seek to sell their shares, leading to a lack of buyers and causing a price
            saturation with a lower bound. This, in turn, results in some streams being blocked for several days,
            preventing their use in other symbols for profit generation. These challenges significantly impacted the
            algorithm's performance.
            <br/><br/>
            As suggested earlier, the most crucial factor in this type of work is not finding the best machine to
            predict the future price; rather, it is more important to implement strategies and methods for
            decision-making based on the current situation. Reinforcement learning may offer a promising avenue. By
            defining agents capable of taking actions like creating and canceling orders, and modeling rewards as the
            profit from trading within a specific time period based on probabilities generated by the Signal Generator,
            a more adaptive approach could be achieved. Although we attempted to pursue this direction, time constraints
            hindered us from completing the task, ultimately leading to the collapse of our startup. */}
		</div>
	);
};

const ProjectBody02 = () => {
	return (
		<div>
			<em style={{ fontSize: "15px" }}>Last updated: 1 Nov, 2025</em>
			<div style={{ margin: "0 auto", textAlign: "center" }}>
				<figure>
					<Zoom>
						<img
							className="image-1"
							alt="Image2Latex Logo"
							src="../MRI_IP_Project.png"
						/>
					</Zoom>
					<figcaption style={{ fontSize: "15px" }}>
						<em>Building...</em>
					</figcaption>
				</figure>
			</div>
			Building....
			{/* One of the most challenging aspects of writing an academic paper with LaTeX is inserting mathematical
            equations and formulas. One approach is to write the formula on a piece of paper, take a picture of it, and
            then use Optical Character Recognition (OCR) engines to convert it into a typed equation. While there are
            already OCR engines that can convert handwritten text into typed text, there are a few examples that
            specialize in handling mathematical images.
            <br/><br/>
            In this article, I am going to explain how I utilized deep neural networks to convert a computer-generated
            image of a mathematical formula into LaTeX code. It is worth noting that handling handwritten images
            presents more challenges. I employed innovative methods for this task as part of my deep learning course
            project.
            <br/><br/>
            The main concept is attention. When writing a formula, the process involves reading from left to right,
            writing each token while maintaining focus on that token and its position within the entire formula. This
            approach is akin to the process of generating a caption for a figure. In a <a target="_blank"
                                                                                          rel="noreferrer"
                                                                                          href="https://arxiv.org/abs/1502.03044">specific
            method</a>, a
            CNN network extracts the feature map of an image, followed by the use
            of a language model with an attention mechanism. This model sequentially generates each token while
            selectively focusing on different locations within the feature map of the image.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Show, Attend and Tell"
                            src="../projects_image_to_latex_02.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Show, Attend and Tell from <a target="_blank" rel="noreferrer"
                                                      href="https://zhuanlan.zhihu.com/p/32333802">this link</a>
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            In this approach, the provided image undergoes initial processing through a CNN network, extracting the
            image's feature map. This map serves as the foundation for the initial state of the RNN. At each state, the
            RNN generates the predicted output vocabulary and produces a map of the same size as the feature map. This
            map functions as the attention mechanism. The map is multiplied by the extracted feature map, and the result
            becomes the input for the subsequent RNN step. Consequently, in generating each vocabulary element, both the
            previous vocabulary and the attention result are fed into the RNN. The expectation is that, for each
            vocabulary element, the feature map adjusts the pixels corresponding to the relevant part of the image,
            effectively filtering out irrelevant areas.
            <br/><br/>
            While the idea is promising, there is speculation regarding the actual functioning of attention as
            anticipated. The absence of a guarantee that the attention map consistently selects the relevant part of the
            image for generating each output vocabulary raises concerns. Additionally, the entire structure is trained
            solely based on the loss computed at the output, without assurance that every aspect of the structure is
            effectively trained, especially considering the issue of gradient vanishing in RNNs. In response to these
            concerns, I have modified the aforementioned structure with the following suggestions:
            <br/>
            <ul>
                <li>
                    Utilize Inception modules as CNN blocks due to their capability to handle different sizes of various
                    tokens in the input image.
                </li>
                <li>
                    Pre-train the Inception modules in another task with specific labeling. Enforce the Inception
                    network to initially identify the bounding box of the entire given image, ensuring that the
                    attention mechanism aligns with expectations.
                </li>
                <li>
                    Apply a Transformer network with skip-gram embedding and the bounding box output of the Inception
                    network to translate math tokens (markups) into math characters using an attention mechanism.
                </li>
            </ul>
            <br/>
            <div className="heading-1">Data Exploration</div>
            <br/><br/>
            The dataset, consisting of nearly 140k samples of math images paired with LaTeX code, can be accessed
            through <a target="_blank" rel="noreferrer" href="https://untrix.github.io/i2l/140k_download.html">this
            link</a>.
            The LaTeX code corpus comprises around 550 unique vocabularies,
            such as &#123;, Q, ^, \mu, and \frac. While the vocabulary size may not be extensive,
            the challenge is more complex than initially anticipated. Some vocabularies are combined to form
            intricate markups, such as \begin&#123;array&#125;, \begin&#123;matrix&#125;,
            and \begin&#123;tabular&#125;.
            Additionally, certain vocabularies lack a corresponding markup as output, including \ref and \label.
            Some are positioned above, below, or in proximity to the next vocabulary,
            such as \sqrt, \widehat, \widetild, \matrix, \over, \fbox, \underline,
            and \choose.
            Lastly, there are symbols that influence the appearance of the succeeding vocabulary output, such
            as \mathrm, \mathsf, \mathcal, \mbox, \rm, \scriptscriptstyle, \sf, \textbf,
            and \textit.
            <br/><br/>
            <div className="heading-1">Unique Markups</div>
            <br/><br/>
            By examining each combination of distinct markups, particularly those mentioned in the previous section, I
            identified 2608 unique visual markups. To achieve this, I initially extracted all 550 unique vocabularies
            from the training labels. Subsequently, I checked each vocabulary in various examples to pinpoint those
            causing the above difficulties. Finally, I created a simple .tex file template and used a for loop to
            automatically compile the .tex document, saving the output images in a directory. The following image
            displays some samples.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Example unique markups"
                            src="../projects_image_to_latex_03.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Example unique markups
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Finding Bounding Boxes</div>
            <br/><br/>
            After identifying all unique markups, I proceeded to determine the location of each markup and annotated it
            with a rectangular bounding box. This step is crucial for defining attention blocks. Indeed, what I did here
            is extracting a guide for the attention mechanism. I will delve into this in more detail later; for now,
            let's focus on the task of locating bounding boxes for all markups.
            <br/><br/>
            Traditional image processing methods, particularly the template matching approach, prove to be a valuable
            tool for this purpose. I utilized a well-known technique called template matching, which involves locating a
            specific pattern within a given image. In our case, the objective is to identify unique markups in the input
            image. The process entails sliding the unique markup across the image, calculating the correlation between
            the markup and each location it passes through, resulting in a correlation matrix. The peak index in this
            matrix corresponds to the location where the markup most closely resembles the pattern at that point. To
            enhance this process, I employed a more sophisticated method known as <a target="_blank" rel="noreferrer"
                                                                                     href="https://docs.opencv.org/4.x/df/dfb/group__imgproc__object.html">TM_SQDIFF_NORMED</a>,
            which normalizes these correlations.
            <br/><br/>
            However, there are two challenges with this method. First, the size of the specific markup in the image may
            deviate from the original markup. Second, executing this method for all 2608 unique markups proves to be
            highly time-consuming. Fortunately, solutions exist to address these issues.
            <br/><br/>
            To address the issue of varying sizes, we can employ template matching with different sizes of the markup.
            Estimating the range of sizes involves comparing the size of the image with that of the markup.
            For the second problem, using the label of the image in the dataset (the LaTeX code) proves to be a useful
            strategy. Extracting the used markups and their frequency of occurrence can be easily achieved by reading
            the LaTeX code. It is important to be cautious with specific vocabularies such as \mathcal and \textbf that
            generate combined markups.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="bounding boxes"
                            src="../projects_image_to_latex_04.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Sample identified bounding boxes in the images
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Assigning a Vector for Each Markup</div>
            <br/><br/>
            The objective in this section is to assign a compact vector to each of the 2608 unique markups. These
            vectors will be utilized in the subsequent pre-training of the CNN network.
            <br/><br/>
            The basic idea involves using dimensionality reduction tools, such as PCA, to condense the pixel space to a
            few numbers. However, it is crucial to note that the distribution of pixels within the markups does not
            adhere to a normal distribution. In reality, markups are grouped together, with some highly correlated
            within their respective group but distant from other correlated groups. Consequently, PCA is not the most
            suitable approach for this purpose.
            <br/><br/>
            For this purpose, I employed a novel approach. Initially, I utilized KMeans to segregate highly correlated
            images into different clusters. Subsequently, I applied PCA independently in each cluster. The final vector
            for each markup is generated by concatenating two vectors. The first component is the center of the
            corresponding cluster, reduced to 5 dimensions by applying PCA to all central points. The second component
            is the 11-dimensional vector obtained by applying PCA to the markup within its cluster. Consequently, each
            16-dimensional vector for a markup encapsulates two key aspects: 1) the cluster to which the markup belongs,
            and 2) the precise location of the markup within that cluster.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="pca markup"
                            src="../projects_image_to_latex_05.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Each markup is represented by a dot in a two-dimensional space, with each dot corresponding to
                        the assigned vector for the respective markup.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Training CNN Network</div>
            <br/><br/>
            As a recap, our goal is to implement an attention mechanism to address two key questions: 1. What markups
            constitute the given image? 2. Where is the location of each markup within the image? With the completion of
            the preceding steps, we have already addressed these questions. The 16-dimensional vector assigned to each
            markup represents the markup itself, while the bounding box identified for each markup indicates its
            location. With this information, we are now ready to proceed with the pre-training of our CNN network.
            <br/><br/>
            I utilized inception modules at different layers of the network because this architecture is well-suited for
            handling different sizes of input images. Given that distinct markups are presented in the input image with
            varying sizes, the network needs to be capable of effectively capturing these differences.
            <br/><br/>
            I created mock labels for training this network. Assuming the input image size is 60x360, I generated a
            label map with dimensions 60x360x16. Let's take the example of the markup 'A' appearing in the input image
            with a bounding box from point [30, 110] to point [40, 120]. The label map in this specified rectangle is
            filled with the vector assigned to markup 'A.' This process is iteratively applied to all markups in the
            image. Then, the network is trained to predict this label map.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="sample BB"
                            src="../projects_image_to_latex_06.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The sample image is displayed at the top, accompanied by its mock label map on the left and the
                        CNN network's prediction on the right. Each row in the label map and prediction corresponds to a
                        specific third dimension (16 layers overall).
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            By employing this approach, we ensure that the CNN network is pre-trained to attend to various locations
            within the image. The network is now prepared to be integrated into the attention mechanism to extract the
            LaTeX code.
            <br/><br/>
            <div className="heading-1">Transformer</div>
            <br/><br/>
            In the final step, a language model is employed to translate these markups considering their respective
            locations in the image. The Transformer architecture, known for its reliance on the attention mechanism, is
            chosen for this task. The extracted feature maps from the CNN inception network serve as the input for the
            Transformer, which sequentially generates each vocabulary at every time step.
            <br/><br/>
            In the embedding section of the Transformer, I employed the skip-gram method to pre-train the embedding
            matrix. Additionally, certain parts of the network, notably the positional encoding block, were modified to
            accommodate three-dimensional images. The provided image illustrates a sample input image along with its
            predicted LaTeX code.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="transformer results"
                            src="../projects_image_to_latex_07.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Results of Transformer network with attention to different locations for generating markups.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Conclusion</div>
            <br/><br/>
            This article addresses the difficulty of integrating mathematical equations into LaTeX papers and proposes a
            solution using deep NNs with attention mechanisms. I modified the structure to enhance attention and
            suggested using Inception modules in CNN blocks. I presented challenges in handling unique vocabularies. The
            process involves finding bounding boxes, assigning vectors to markups, and training a CNN network. The final
            step employs a Transformer architecture to translate markups into LaTeX code based on their locations in the
            image.
            <br/><br/>
            In my opinion, all steps were executed successfully except for the final one. The Transformer network did
            not generate the codes as anticipated. For future work, readers are encouraged to propose improved
            structures to enhance the performance of the Transformer network. */}
		</div>
	);
};

const ProjectBody03 = () => {
	return (
		<div>
			<em style={{ fontSize: "15px" }}>Last updated: 3 Nov, 2025</em>
			<div style={{ margin: "0 auto", textAlign: "center" }}>
				<figure>
					<Zoom>
						<img
							className="image-2"
							alt="Zara Logo"
							src="../Zara-logo.jpg"
						/>
					</Zoom>
					<figcaption style={{ fontSize: "15px" }}>
						<em>{/* later */}</em>
					</figcaption>
				</figure>
			</div>
			<br />
			<br />
			{/* <div className="heading-1">Redesign of Zara.com</div> */}
			<br />
			<br />
			<div className="heading-1">Skills Demonstrated:</div>
			<br />
			We primarily used <strong>Figma</strong> and{" "}
			<strong>critical analysis</strong> to identify usability issues in
			the existing Zara website. Then, by applying the six core design
			principles, we refined and improved the interface to make it more
			intuitive and user-friendly.
			<br />
			<br />
			<b>Wireframing:</b> Because the website already had an established
			structure, our focus was on <strong>refining</strong> rather than
			reinventing it. We reviewed key pages to identify confusing layouts
			and improved them to create a clearer, smoother, and more enjoyable
			user experience.
			<br />
			<br />
			<b>Heuristic Evaluation:</b> We applied the main design principles—{" "}
			<strong>
				visibility, constraints, feedback, consistency,
				affordance/signifier, and mapping
			</strong>{" "}
			— to evaluate the original interface. For each principle, we
			documented specific issues, explained why they violated the
			principles, and used these findings to guide our redesign decisions.
			<br />
			<br />
			<b>User Research:</b> To test whether our redesign worked better
			than the original, we presented it during a{" "}
			<strong>critique session</strong> and gathered peer feedback. The
			responses were positive—reviewers found our redesign clearer, more
			logical, and well-aligned with the design principles we aimed to
			demonstrate.
			<br />
			<br />
			<br />
			From <strong>October 28, 2025</strong> to{" "}
			<strong>November 3, 2025</strong> is indeed <strong>6 days</strong>{" "}
			in duration.
			<br />
			<br />
			We redesigned this website as a team of two. I worked on it with my
			classmate, Thi Phuong Chi Vo, at UMBC.
			<br />
			<br />
			We discussed almost all parts together, but my main focus was on
			Constraints, Feedback, and Mapping. I also wrote the overall report
			myself.
			<br />
			<br />
			The main tool we used was <b>Figma</b>, and we also used some{" "}
			<b>generative AI tools</b> to solve the questions that came up while
			working with Figma.
			<br />
			<br />
			<br />
			<br />
			<div className="heading-1">Context and problem statement</div>
			<br />
			When you arrive at this page, you might ask yourself: Why did we
			redesign this website? Are we really better than the original
			designers? Well — with no offense intended — we have to say yes!
			<br />
			<br />
			As we explored Zara's website, we were surprised to find that a
			company worth over $100 billion could have so many design flaws. So,
			we decided to redesign it based on{" "}
			<b>six core design principles </b>, including:
			<br />
			<ul>
				<li>
					<strong>
						<a
							href="#visibility"
							onClick={(e) => {
								e.preventDefault(); // don't let the router change the URL
								const el =
									document.getElementById("visibility");
								if (el) {
									el.scrollIntoView({ behavior: "smooth" });
								}
							}}
						>
							Visibility
						</a>
					</strong>
				</li>
				<li>
					<a
						href="#constraints"
						onClick={(e) => {
							e.preventDefault();
							const el = document.getElementById("constraints");
							if (el) el.scrollIntoView({ behavior: "smooth" });
						}}
					>
						<strong>Constraints</strong>
					</a>
				</li>
				<li>
					<a
						href="#feedback"
						onClick={(e) => {
							e.preventDefault();
							const el = document.getElementById("feedback");
							if (el) el.scrollIntoView({ behavior: "smooth" });
						}}
					>
						<strong>Feedback</strong>
					</a>
				</li>
				<li>
					<a
						href="#consistency"
						onClick={(e) => {
							e.preventDefault();
							const el = document.getElementById("consistency");
							if (el) el.scrollIntoView({ behavior: "smooth" });
						}}
					>
						<strong>Consistency</strong>
					</a>
				</li>
				<li>
					<a
						href="#signifier"
						onClick={(e) => {
							e.preventDefault();
							const el = document.getElementById("signifier");
							if (el) el.scrollIntoView({ behavior: "smooth" });
						}}
					>
						<strong>Affordance/Signifier</strong>
					</a>
				</li>
				<li>
					<a
						href="#mapping"
						onClick={(e) => {
							e.preventDefault();
							const el = document.getElementById("mapping");
							if (el) el.scrollIntoView({ behavior: "smooth" });
						}}
					>
						<strong>Mapping</strong>
					</a>
				</li>
			</ul>
			<br />
			<br />
			<div className="heading-1">Overview of Design Process</div>
			<br />
			In the following sections, we analyze one design flaw related to
			each of these principles and explain our redesign approach.
			<br />
			<br />
			<br />
			<div id="visibility" className="heading-1">
				1. Visibility:
			</div>
			<br />
			As you may guess from the word visibility, it’s all about making
			things easy to see and understand (Kotturi, 2025a). When users land
			on a page, they should instantly know what to do next — no guessing
			games, no hidden buttons.
			<br />
			<br />
			Now, let’s look at Zara’s product page.
			<br />
			<br />
			When I tried to buy a short tie dress, I clicked on the product and…
			surprise! No sizes showed up. I was like, “Okay… how do I pick my
			size now?” After a few seconds of clicking around — boom! — I
			realized the sizes only appeared <b>AFTER</b> I hit “Add.” Not cool.
			I had no clue what size I was even adding!
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 1" src="../image_1.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 1. Visibility issue_1</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 2" src="../image_2.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 2. Visibility issue_2</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			So, I decided to fix this mystery.
			<br />
			<br />
			In my redesign, all the available sizes show up right away — no
			detective work required. That way, shoppers can instantly see which
			sizes are in stock before adding anything to the cart.
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 3" src="../image_3.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 3. Visibility redesign_1</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 4" src="../image_4.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 4. Visibility redesign_2.</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 5" src="../image_5.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 5. Visibility redesign_3</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			This small change makes a big difference. Shoppers don’t have to
			play hide-and-seek with the size menu anymore — everything they need
			is visible at the right time, which makes the experience faster,
			smoother, and way less confusing.
			<br />
			<br />
			<br />
			<br />
			<div id="constraints" className="heading-1">
				2. Constraints
			</div>
			<br />
			Let’s be honest—as humans, we don’t like being limited. We want
			freedom, choices, and flexibility. But in design, a little
			limitation can actually save the day.
			<br />
			<br />
			In UX, constraints are the good kind of limits—they guide users,
			prevent errors, and stop things from going horribly wrong (Kotturi,
			2025a). In short, constraints help users do the right thing (and
			avoid the wrong ones).
			<br />
			<br />
			Now, let’s take a look at the current Zara website.
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 6" src="../image_6.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 6. Constraints issue_1</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			When I tried logging into my account with the wrong password several
			times, the system just… let me keep trying. No warning, no lockout,
			not even a polite “Hey, are you okay?” 😅
			<br />
			<br />
			This is risky because it leaves the account open to something called
			a brute force attack, where someone (or a bot) keeps guessing
			passwords until they hit the jackpot.
			<br />
			<br />
			To fix this, I designed a new size chart to easily use:
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 7" src="../image_7.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 7. Constraint _ redesign</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			In my redesign, after a few failed login attempts, the system
			temporarily locks the account and sends a helpful email saying, “We
			noticed unusual activity — please reset your password or confirm
			this was you.”
			<br />
			<br />
			This way, users stay protected, attackers get blocked, and everyone
			sleeps better at night.
			<br />
			<br />
			By adding this constraint, we’re not limiting the user — we’re
			protecting them. Sometimes, good design is about saying: “Nope, you
			can’t do that — and that’s for your own good.”
			<br />
			<br />
			<br />
			<br />
			<div id="feedback" className="heading-1">
				3. Feedback
			</div>
			<br />
			Feedback can be a double-edged sword. Sometimes it’s great — like
			when someone says, “Next time, add a little more salt, it’ll taste
			amazing!” But sometimes… not so great — like when someone comments,
			“Wow, you’ve gained weight!” (In that case, we just smile and say,
			“Thanks, I have a mirror at home.” 🙄)
			<br />
			<br />
			In design, feedback works the same way: it can be annoying or
			extremely helpful — depending on how it’s delivered. The key idea is
			simple: Communicate an immediate and appropriate response to the
			user’s action (Kotturi, 2025a).
			<br />
			<br />
			Now, let’s see what’s happening on the Zara website.
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 8" src="../image_8.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 8. Feedback issue</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			When I hover my cursor over the “Add” button, nothing happens. No
			color change, no animation, not even a slight movement. It’s like
			the button is saying, “Don’t touch me.” 🥲 It may seem like a small
			detail, but in online shopping, these tiny cues make a big
			difference. A dull, unresponsive button feels uninviting and doesn’t
			motivate the user to click.
			<br />
			<br />
			So, we gave it some life!
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 9" src="../image_9.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em> Figure 9. Feedback redesign</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			In the redesign, when users hover or click, the button
			responds—maybe by changing color, slightly growing, or showing a
			ripple animation. Now, the shopper feels the interaction. It’s like
			the website is saying, “Yes! I heard you! Let’s add this to your
			cart!”
			<br />
			<br />
			This kind of responsive feedback not only makes the interface feel
			alive but also reassures users that their action worked.
			<br />
			<br />
			<strong>
				Yaaay! Another little detail that makes the experience smoother,
				friendlier, and more human-centered.
			</strong>
			<br />
			<br />
			<br />
			<br />
			<div id="consistency" className="heading-1">
				4. Consistency
			</div>
			<br />
			You know that comforting feeling when everything just matches—like
			when your socks actually pair up after laundry day? That’s the magic
			of consistency. 🪄🤠
			<br />
			<br />
			In UI design, consistency means using familiar patterns, layouts,
			and elements so that users don’t have to relearn how to do things
			every time. When similar actions look and behave the same way, users
			feel confident and in control.
			<br />
			<br />
			Now, let’s talk about Zara.
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 10" src="../image_10.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 10. Consistency issue </em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			When browsing different sections of the Zara website, I noticed
			something odd. Some pages — like Zara Origins—look like they belong
			to a completely different brand. The fonts, spacing, and even the
			layout don’t match the main Zara interface. Suddenly, it feels like
			you’ve been teleported to another website!
			<br />
			<br />
			This inconsistency can confuse users and make them question whether
			they’re still in the same shopping environment. It breaks trust and
			disrupts the flow—definitely not the sleek Zara experience we
			expect.
			<br />
			<br />
			So, we stepped in to fix it.
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 11" src="../image_11.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em> Figure 11. Consistency redesign </em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			In this redesign, we unified the typography, spacing, and layout
			system across all pages. Now, every section feels connected—visually
			and emotionally. The whole site speaks one consistent design
			language that matches Zara’s minimalist and elegant style.
			<br />
			<br />
			Consistency might seem like a small detail, but it’s what makes a
			brand feel trustworthy and professional. When everything feels
			familiar, users relax—and that’s when the magic (and the shopping)
			happens.
			<br />
			<br />
			<br />
			<br />
			<div id="signifier" className="heading-1">
				5. Signifier
			</div>
			<br />
			Yup—we humans love to feel in control. But let’s be honest,
			sometimes we need a little hint to figure out what to do next.
			That’s where signifiers come in—those tiny, visible cues that
			whisper, “Hey, click here!” or “Swipe me!”
			<br />
			<br />
			In design terms, a signifier is any perceivable indicator that shows
			what action is possible and how to take it (Kotturi, 2025b). Simple,
			right? But it’s incredibly powerful. Without clear signifiers, users
			can feel lost—like standing in front of a door and wondering whether
			to push, pull, or just give up and go home.
			<br />
			<br />
			Now, back to Zara’s website.
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 12" src="../image_12.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 12. Signifier issue </em>
					</figcaption>
				</Zoom>
			</figure>
			When I experienced this website, I recognized that the search bar
			lacks some identifying markup (e.g., a border, a “Search”
			placeholder, or a magnifying glass icon). Thus, I think users are
			unaware that they can enter text there, even though the
			functionality exists.
			<br />
			<br />
			To fix it, I only changed 2 small things: a rectangular border to
			highlight the “Search” bar and a metaphor whose image is a
			magnifying glass.
			<br />
			<br />
			From our redesign, users immediately recognize that this is where to
			enter the search bar. Moreover, the icons and placeholders act as
			clear signifiers, reducing the time it takes to recognize the
			function, resulting in an improved intuitive experience, especially
			for new or less tech-savvy users.
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 13" src="../image_13.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 13. Signifier redesign</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			<br />
			<br />
			<div id="mapping" className="heading-1">
				6. Mapping
			</div>
			<br />
			“Guide me the way I desire” — that’s basically what mapping is all
			about. In simple terms, mapping refers to how well the design’s
			controls and visuals correspond to their effects — or, put
			differently, how easily users can predict what will happen when they
			interact with something (Kotturi, 2025b).
			<br />
			<br />
			Let’s look at Zara’s website through this lens.
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 14 " src="../image_14.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 14. Mapping issue.</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			When I clicked on Basic T-Shirts, I expected to see… Well, basic
			t-shirts. But what popped up instead? A woman wearing a coat — maybe
			with a basic shirt underneath (hard to tell!). I totally respect the
			artistic direction, but from a user’s point of view, this doesn’t
			really “map” to what I clicked on.
			<br />
			<br />
			And the confusion didn’t stop there. As I scrolled down, the page
			first showed me completely different products — long sleeves,
			patterned shirts — and only after scrolling more did the actual
			basic shirts appear. That breaks the mapping principle, because the
			visual response doesn’t match the user’s mental expectation.
			<br />
			<br />
			You can see this phenomenon in this{" "}
			<a
				href="https://drive.google.com/file/d/18e69lzKuEjiBPTGxneYGH89oVTcGLLrN/view?usp=sharing"
				target="_blank"
				rel="noreferrer"
			>
				VIDEO
			</a>
			<br />
			<br />
			So, we fixed it!
			<br />
			<br />
			<figure>
				<Zoom>
					<img alt="Figure 15" src="../image_15.png" width="100%" />
					<figcaption style={{ textAlign: "center" }}>
						<em>Figure 15. Mapping redesign</em>
					</figcaption>
				</Zoom>
			</figure>
			<br />
			<br />
			In the redesign, when users click on Basic T-Shirts, the first thing
			they see is a styled model clearly wearing the basic shirt —
			followed immediately by colorful variations of the same item. This
			creates a logical flow between the user’s action and the on-screen
			result.
			<br />
			<br />
			Now the interface feels natural and predictable — no more
			“scroll-and-guess” moments. Good mapping makes users feel like the
			website understands them and knows what they came for.
			<br />
			<br />
			<br />
			<br />
			<div className="heading-1">Outcome</div>
			<br />
			The redesign we created for Zara’s website didn’t fix every
			issue—there were actually quite a few—but we focused on six major
			ones based on the core design principles discussed earlier.
			<br />
			<ul>
				<li>
					<strong>We improved visibility</strong> by showing the size
					options immediately, instead of requiring users to click
					“Add” before selecting a size.
				</li>
				<li>
					<strong>We enhanced constraints</strong> by adding a
					brute-force protection feature for repeated failed logins,
					keeping customer accounts safer.
				</li>
				<li>
					<strong>We improved feedback</strong> by adding small visual
					responses—like color or motion changes—when users hover over
					buttons such as “Add to Cart.”
				</li>
				<li>
					<strong>We strengthened consistency</strong> by unifying the
					typography, layout, and visual style across Zara’s different
					regional websites.
				</li>
				<li>
					<strong>We refined signifiers</strong> by making the search
					box more noticeable and adding a magnifying-glass icon so
					users instantly recognize its purpose.
				</li>
				<li>
					<strong>Finally, we enhanced mapping</strong> so that when
					users click on a category—like “Basic T-shirts”—they see the
					correct products right away, without having to scroll past
					unrelated items.
				</li>
			</ul>
			<br />
			We’ve included examples of our redesigned screens above, and the
			full interactive version is available on Figma.{" "}
			<a
				href="https://www.figma.com/design/DcxAd4ddiWDa0uMUXCp6np/Chi-Vo---Sana-Nahvi---Zara-Redesign?node-id=71-65&t=vCKa96VL39p2tT9X-1"
				target="_blank"
				rel="noreferrer"
			>
				Link
			</a>
			<br />
			<br />
			<br />
			<br />
			<div className="heading-1">Reflection or lessons learned</div>
			<br />
			During this project, we learned much more than just how to use Figma
			for redesigning a website. We started to understand what really
			makes a website feel easy, clear, and enjoyable to use. Applying the
			main design principles—like visibility, feedback, constraints, and
			mapping—helped us see how small changes can make a big difference in
			user experience.
			<br />
			<br />
			Another big thing we learned was how to think like real designers.
			We learned to look at a website from the user’s point of view, find
			what’s confusing, and think of better ways to fix it. Honestly, at
			first, I didn’t believe a big company like Zara could have any
			usability issues—but we found several! It was a great reminder that
			even famous brands can always improve their design.
			<br />
			<br />
			<br />
			<br />
			<div className="heading-1">Reference </div>
			<ul>
				<li>
					Kotturi, Y. (2025, 9 22).{" "}
					<em>Interaction design rules – Part 1</em> [Slide
					presentation]. Baltimore, Maryland, USA.
				</li>
				<li>
					Kotturi, Y. (2025, 9 29).{" "}
					<em>Interaction design rules – Part 2</em> [Slide
					presentation]. Baltimore, Maryland, USA.
				</li>
				<li>
					OpenAI. (2025). <em>ChatGPT [Large language model]</em>.
				</li>
			</ul>
		</div>
	);
};

const ProjectBodies = [ProjectBody01, ProjectBody02, ProjectBody03];
export default ProjectBodies;
